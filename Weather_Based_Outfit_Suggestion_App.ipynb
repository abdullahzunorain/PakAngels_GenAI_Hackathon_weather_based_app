{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCt61zRG4DuBg24M9muz3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71c9267c227048c0a127332c934c596f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0b4c4c005124d63b84c9a77730a00ef",
              "IPY_MODEL_c4160269801b4c21a3df194c3860592d",
              "IPY_MODEL_342f9da828454a008c56d822263a279b"
            ],
            "layout": "IPY_MODEL_1d0e163975d34d7bb8b3826af2153584"
          }
        },
        "b0b4c4c005124d63b84c9a77730a00ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_192209c101cb407da5a6116d08551a35",
            "placeholder": "​",
            "style": "IPY_MODEL_9ea8559f8d3a4d0fa05ccbba602d23c1",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "c4160269801b4c21a3df194c3860592d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4425695aff847fa8258a328d5b4aaa4",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03e7a17e9ecd4981a15c00e081ca8efb",
            "value": 7
          }
        },
        "342f9da828454a008c56d822263a279b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bc13f08a0944eb9aeb130d9df62f647",
            "placeholder": "​",
            "style": "IPY_MODEL_612f262802b9475daf53dc63dc419b6a",
            "value": " 7/7 [00:02&lt;00:00,  1.96it/s]"
          }
        },
        "1d0e163975d34d7bb8b3826af2153584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192209c101cb407da5a6116d08551a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea8559f8d3a4d0fa05ccbba602d23c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4425695aff847fa8258a328d5b4aaa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e7a17e9ecd4981a15c00e081ca8efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bc13f08a0944eb9aeb130d9df62f647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "612f262802b9475daf53dc63dc419b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c19cde8d90f34f159bf36a2ce7339cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b6f070ceffa48a782bf36dd485eff79",
              "IPY_MODEL_77612db84eab4edbb835ae0cf313d992",
              "IPY_MODEL_2eee9d3cbf2143aeb8215e03a6416ca7"
            ],
            "layout": "IPY_MODEL_e44721a5fed04e689f73292b286e5dad"
          }
        },
        "7b6f070ceffa48a782bf36dd485eff79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_264274334b6e48e4b7e510b924a80b3b",
            "placeholder": "​",
            "style": "IPY_MODEL_ba13ff27f8e342c486a58ab92f11c123",
            "value": "100%"
          }
        },
        "77612db84eab4edbb835ae0cf313d992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fafb80046e704d65b49846d7303d10ce",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6251ea311094901b5a55aa0b2abea7d",
            "value": 50
          }
        },
        "2eee9d3cbf2143aeb8215e03a6416ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcdd2d34c1f24bf4901a1267fcf6e671",
            "placeholder": "​",
            "style": "IPY_MODEL_a17517ed26654e038d9e807fa37d1cff",
            "value": " 50/50 [00:22&lt;00:00,  2.36it/s]"
          }
        },
        "e44721a5fed04e689f73292b286e5dad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "264274334b6e48e4b7e510b924a80b3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba13ff27f8e342c486a58ab92f11c123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fafb80046e704d65b49846d7303d10ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6251ea311094901b5a55aa0b2abea7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcdd2d34c1f24bf4901a1267fcf6e671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17517ed26654e038d9e807fa37d1cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahzunorain/Making_an_end-to-end-project_on_colab_using_streamlit/blob/main/Weather_Based_Outfit_Suggestion_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq requests gradio\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIPlfxCnTJGu",
        "outputId": "ac88a585-ae92-47c6-c16d-43bd0b1e34d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.5.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, safehttpx, groq, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.4 ffmpy-0.4.0 gradio-5.5.0 gradio-client-1.4.2 groq-0.11.0 huggingface-hub-0.26.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.3 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_TOKEN = userdata.get('GROQ_API_TOKEN')\n"
      ],
      "metadata": {
        "id": "6SigWzhfNXEg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "WeatherApp = userdata.get('WeatherApp')\n"
      ],
      "metadata": {
        "id": "o3moi6RxNbKq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the GROQ API Token and WeatherApp API Key\n",
        "GROQ_API_TOKEN = userdata.get('GROQ_API_TOKEN')\n",
        "WeatherApp = userdata.get('WeatherApp')\n",
        "\n",
        "# Confirm that the keys are accessed (print statements should be avoided for sensitive data in production)\n",
        "print(\"GROQ API Token:\", \"Loaded Successfully\" if GROQ_API_TOKEN else \"Not Found\")\n",
        "print(\"WeatherApp Key:\", \"Loaded Successfully\" if WeatherApp else \"Not Found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EqmVTaLNfUj",
        "outputId": "dd864484-eb45-4c80-a1e2-7b05cf6c9537"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GROQ API Token: Loaded Successfully\n",
            "WeatherApp Key: Loaded Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_weather_data(location, api_key):\n",
        "    # OpenWeatherMap API endpoint\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={api_key}&units=metric\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return {\n",
        "            \"location\": data[\"name\"],\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"weather\": data[\"weather\"][0][\"description\"],\n",
        "            \"humidity\": data[\"main\"][\"humidity\"],\n",
        "            \"wind_speed\": data[\"wind\"][\"speed\"]\n",
        "        }\n",
        "    else:\n",
        "        return {\"error\": data.get(\"message\", \"An error occurred\")}\n"
      ],
      "metadata": {
        "id": "xG2WrdA-TTHU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "# Setup API key\n",
        "client = Groq(api_key=GROQ_API_TOKEN) #os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "def generate_outfit_suggestion(weather_data):\n",
        "    prompt = f\"Suggest an outfit for a person based on the following weather data: {weather_data}\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        model=\"llama3-8b-8192\"\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "95ZyeuPCTZ2F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def suggest_outfit(location, weather_api_key):\n",
        "    # Get weather data\n",
        "    weather_data = get_weather_data(location, weather_api_key)\n",
        "\n",
        "    if \"error\" in weather_data:\n",
        "        return weather_data[\"error\"]\n",
        "\n",
        "    # Generate outfit suggestion using LLAMA\n",
        "    outfit_suggestion = generate_outfit_suggestion(weather_data)\n",
        "    return outfit_suggestion\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=suggest_outfit,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Your Location\"),\n",
        "        gr.Textbox(label=\"OpenWeatherMap API Key (Optional)\", type=\"password\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    live=True\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "c6P3_oajTjUQ",
        "outputId": "ecd4a022-2053-4aaf-a744-c3163bbcb5b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6fe7d85130e8567fb1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6fe7d85130e8567fb1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ec-WldFTjXO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "IeaQdya5UGBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "application"
      ],
      "metadata": {
        "id": "MX5Vt2c2VJDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install groq requests streamlit gradio\n",
        "\n",
        "# Import Libraries and Set API Keys\n",
        "import os\n",
        "import requests\n",
        "from groq import Groq\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the secret keys securely\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_TOKEN')\n",
        "WEATHER_API_KEY = userdata.get('WeatherApp')\n",
        "\n",
        "# Set environment variables for the APIs\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "\n",
        "# Fetch Weather Data from OpenWeatherMap\n",
        "def get_weather_data(location, api_key):\n",
        "    \"\"\"Fetch weather data from OpenWeatherMap API\"\"\"\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={api_key}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return {\n",
        "            \"location\": data[\"name\"],\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"weather\": data[\"weather\"][0][\"description\"],\n",
        "            \"humidity\": data[\"main\"][\"humidity\"],\n",
        "            \"wind_speed\": data[\"wind\"][\"speed\"]\n",
        "        }\n",
        "    else:\n",
        "        return {\"error\": data.get(\"message\", \"An error occurred\")}\n",
        "\n",
        "# Generate Outfit Suggestions using Groq LLAMA model\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "def generate_outfit_suggestion(weather_data):\n",
        "    \"\"\"Generate outfit suggestion based on weather data using LLAMA model\"\"\"\n",
        "    prompt = f\"Suggest an outfit for a person based on the following weather data: {weather_data}\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        model=\"llama3-8b-8192\"  # You can use a smaller model if needed, such as llama3-1b\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# Suggest Outfit Function combining both weather and LLAMA model\n",
        "def suggest_outfit(location, weather_api_key):\n",
        "    \"\"\"Function to combine weather data and Groq model for outfit suggestion\"\"\"\n",
        "    # Get weather data\n",
        "    weather_data = get_weather_data(location, weather_api_key)\n",
        "\n",
        "    if \"error\" in weather_data:\n",
        "        return weather_data[\"error\"]\n",
        "\n",
        "    # Prepare weather data for outfit suggestion\n",
        "    weather_summary = f\"Location: {weather_data['location']}, Temp: {weather_data['temperature']}°C, \" \\\n",
        "                      f\"Weather: {weather_data['weather']}, Humidity: {weather_data['humidity']}%, \" \\\n",
        "                      f\"Wind Speed: {weather_data['wind_speed']} m/s\"\n",
        "\n",
        "    # Generate outfit suggestion using LLAMA model\n",
        "    outfit_suggestion = generate_outfit_suggestion(weather_summary)\n",
        "\n",
        "    return outfit_suggestion\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=suggest_outfit,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Your Location (e.g., London, New York)\", placeholder=\"Location\", lines=1),\n",
        "        gr.Textbox(label=\"OpenWeatherMap API Key\", type=\"password\", placeholder=\"Weather API Key (Optional)\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Real-Time Weather-Based Outfit Suggestion\",\n",
        "    description=\"Enter your location and get outfit suggestions based on current weather conditions.\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "kse8zl5aUGro",
        "outputId": "b2f8698b-a414-4b4e-d27d-9875537c2b64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.3)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.2)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.0 watchdog-5.0.3\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c005aff62b76cf6b53.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c005aff62b76cf6b53.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "hflw5-VAWyyb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KkiPvXVfVLuK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit openai requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c0MOjuuCVLxB",
        "outputId": "f491d448-2f53-45a9-b6ee-2b687a8293e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.40.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import streamlit as st\n",
        "import groq\n",
        "\n",
        "# Function to get weather data from OpenWeatherMap\n",
        "def get_weather_data(city):\n",
        "    api_key = \"-----------------------------\"  # Replace with your OpenWeatherMap API key\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "# Function to parse weather data\n",
        "def parse_weather_data(weather_data):\n",
        "    temperature = weather_data[\"main\"][\"temp\"]\n",
        "    weather_description = weather_data[\"weather\"][0][\"description\"]\n",
        "    return temperature, weather_description\n",
        "\n",
        "# Function to get outfit suggestion using Groq's LLaMA model\n",
        "def get_outfit_suggestion(temperature, description):\n",
        "    model = groq.load_model(\"llama\")  # Load LLaMA model from Groq\n",
        "    prompt = f\"The current weather is {description} with a temperature of {temperature}°C. Suggest an outfit.\"\n",
        "\n",
        "    response = model.generate(\n",
        "        prompt=prompt,\n",
        "        max_length=100,\n",
        "        temperature=0.7,  # Controls creativity of the response\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Streamlit UI for user input\n",
        "st.title(\"Weather-Based Outfit Suggestion App\")\n",
        "\n",
        "city = st.text_input(\"Enter your location:\")\n",
        "\n",
        "if city:\n",
        "    weather_data = get_weather_data(city)\n",
        "\n",
        "    if weather_data[\"cod\"] == 200:\n",
        "        temperature, description = parse_weather_data(weather_data)\n",
        "\n",
        "        # Display current weather info\n",
        "        st.write(f\"Current temperature in {city}: {temperature}°C\")\n",
        "        st.write(f\"Weather: {description}\")\n",
        "\n",
        "        # Get outfit suggestion from Groq's LLaMA model\n",
        "        outfit_suggestion = get_outfit_suggestion(temperature, description)\n",
        "\n",
        "        # Display outfit suggestion\n",
        "        st.write(\"Outfit Suggestion:\")\n",
        "        st.write(outfit_suggestion)\n",
        "\n",
        "        # Display weather icon\n",
        "        icon_code = weather_data[\"weather\"][0][\"icon\"]\n",
        "        icon_url = f\"http://openweathermap.org/img/wn/{icon_code}.png\"\n",
        "        st.image(icon_url)\n",
        "    else:\n",
        "        st.write(\"Could not retrieve weather data. Please check the location.\")\n",
        "\n",
        "# Optional: Add CSS for styling\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .reportview-container {\n",
        "        background: #f5f5f5;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        background-color: #ff5733;\n",
        "        color: white;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R_Q4j1HsXurQ",
        "outputId": "353ba63e-b077-41d5-a51a-29afdfcd7a29"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-09 14:41:25.534 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.604 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-11-09 14:41:25.607 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.610 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.613 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.615 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.617 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.618 Session state does not function when running a script without `streamlit run`\n",
            "2024-11-09 14:41:25.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 14:41:25.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqH1E8qLiSiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "457_kS1bYTWW",
        "outputId": "418c0cfe-52a4-4a5a-f446-f85a1cded612"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.83.23.169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D3we_5sUYWgP",
        "outputId": "3e0cc995-02a7-4e97-b363-28fa86f795e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 41.9M/10.3G [00:02<07:15, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   5% 126M/2.78G [00:02<01:17, 34.3MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  13% 62.9M/492M [00:02<00:18, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 41.9M/335M [00:02<00:12, 23.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 41.9M/335M [00:02<00:12, 23.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 52.4M/10.3G [00:03<12:52, 13.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 52.4M/335M [00:03<00:20, 13.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   5% 136M/2.78G [00:03<02:15, 19.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 52.4M/335M [00:03<00:20, 13.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  15% 73.4M/492M [00:03<00:30, 13.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 62.9M/10.3G [00:03<09:28, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 62.9M/335M [00:03<00:14, 18.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   5% 147M/2.78G [00:04<01:53, 23.2MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  17% 83.9M/492M [00:04<00:22, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 62.9M/335M [00:03<00:14, 18.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 73.4M/10.3G [00:04<09:29, 17.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 73.4M/335M [00:04<00:14, 18.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  19% 94.4M/492M [00:04<00:21, 18.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 83.9M/10.3G [00:04<07:18, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  21% 105M/492M [00:04<00:16, 23.5MB/s] \u001b[A\u001b[A\n",
            "model.safetensors:   6% 157M/2.78G [00:04<02:13, 19.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 83.9M/335M [00:04<00:10, 23.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 73.4M/335M [00:04<00:15, 16.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 94.4M/10.3G [00:04<05:34, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 83.9M/335M [00:04<00:11, 21.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  28% 94.4M/335M [00:04<00:08, 27.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 105M/10.3G [00:04<04:41, 36.1MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  23% 115M/492M [00:05<00:13, 27.0MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   6% 168M/2.78G [00:05<01:56, 22.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 105M/335M [00:04<00:07, 31.0MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 115M/10.3G [00:05<04:27, 37.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  26% 126M/492M [00:05<00:12, 30.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 105M/335M [00:04<00:07, 31.8MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   6% 178M/2.78G [00:05<01:40, 25.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 115M/335M [00:04<00:05, 38.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   7% 189M/2.78G [00:05<01:25, 30.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 126M/10.3G [00:05<04:33, 37.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 115M/335M [00:05<00:06, 33.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  28% 136M/492M [00:05<00:11, 31.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 126M/335M [00:05<00:05, 39.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   7% 199M/2.78G [00:05<01:09, 36.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 136M/10.3G [00:05<03:44, 45.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 126M/335M [00:05<00:05, 38.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  30% 147M/492M [00:05<00:09, 37.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 136M/335M [00:05<00:04, 46.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   8% 210M/2.78G [00:06<02:14, 19.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching 19 files:   0% 0/19 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 147M/335M [00:07<00:14, 13.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 147M/10.3G [00:07<13:14, 12.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  32% 157M/492M [00:07<00:26, 12.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 136M/335M [00:07<00:14, 13.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   8% 220M/2.78G [00:07<02:52, 14.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 157M/335M [00:07<00:10, 17.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  34% 168M/492M [00:08<00:19, 16.7MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   8% 231M/2.78G [00:08<02:10, 19.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 157M/10.3G [00:07<10:08, 16.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 147M/335M [00:07<00:10, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  36% 178M/492M [00:08<00:14, 21.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 168M/335M [00:07<00:07, 22.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 168M/10.3G [00:07<07:47, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   9% 241M/2.78G [00:08<01:43, 24.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 157M/335M [00:07<00:08, 21.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 178M/10.3G [00:09<11:33, 14.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 178M/335M [00:08<00:10, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   9% 252M/2.78G [00:09<02:44, 15.4MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  38% 189M/492M [00:09<00:21, 14.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 168M/335M [00:09<00:11, 14.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 189M/10.3G [00:09<08:54, 18.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 189M/335M [00:09<00:07, 19.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   9% 262M/2.78G [00:09<02:10, 19.3MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  40% 199M/492M [00:09<00:16, 17.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 178M/335M [00:09<00:08, 17.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 199M/10.3G [00:09<07:05, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 199M/335M [00:09<00:05, 23.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  43% 210M/492M [00:09<00:12, 22.9MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  10% 273M/2.78G [00:09<01:45, 23.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 189M/335M [00:09<00:06, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 210M/10.3G [00:09<05:40, 29.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 210M/335M [00:09<00:04, 28.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  45% 220M/492M [00:10<00:09, 29.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 199M/335M [00:09<00:04, 27.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  10% 283M/2.78G [00:10<01:31, 27.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 220M/10.3G [00:09<05:05, 32.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  47% 231M/492M [00:10<00:09, 28.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  66% 220M/335M [00:09<00:04, 27.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 210M/335M [00:10<00:04, 26.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  11% 294M/2.78G [00:10<01:36, 25.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 231M/10.3G [00:10<05:52, 28.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  49% 241M/492M [00:10<00:09, 26.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 231M/335M [00:10<00:04, 25.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 241M/10.3G [00:10<06:24, 26.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 241M/335M [00:10<00:03, 27.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  66% 220M/335M [00:10<00:05, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  51% 252M/492M [00:11<00:08, 27.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 252M/10.3G [00:10<04:57, 33.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  11% 304M/2.78G [00:11<01:55, 21.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 252M/335M [00:10<00:02, 34.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 231M/335M [00:10<00:03, 27.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 262M/10.3G [00:11<04:16, 39.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  53% 262M/492M [00:11<00:07, 31.8MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  11% 315M/2.78G [00:11<01:30, 27.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  78% 262M/335M [00:11<00:01, 39.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 273M/10.3G [00:11<03:29, 47.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 241M/335M [00:11<00:02, 32.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  55% 273M/492M [00:11<00:05, 37.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 283M/10.3G [00:11<03:08, 52.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  12% 336M/2.78G [00:11<01:01, 39.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 273M/335M [00:11<00:01, 43.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 252M/335M [00:11<00:02, 38.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 294M/10.3G [00:11<02:45, 60.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  58% 283M/492M [00:11<00:05, 39.2MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  12% 346M/2.78G [00:11<00:55, 44.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  78% 262M/335M [00:11<00:01, 44.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 283M/335M [00:11<00:01, 45.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 304M/10.3G [00:11<02:26, 68.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  60% 294M/492M [00:11<00:04, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 273M/335M [00:11<00:01, 53.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 294M/335M [00:11<00:00, 52.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  13% 357M/2.78G [00:12<00:51, 46.7MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  62% 304M/492M [00:12<00:03, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 304M/335M [00:11<00:00, 55.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 283M/335M [00:11<00:01, 51.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 325M/10.3G [00:11<02:24, 69.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  13% 367M/2.78G [00:12<00:48, 50.2MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  64% 315M/492M [00:12<00:03, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 294M/335M [00:11<00:00, 51.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  94% 315M/335M [00:11<00:00, 53.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 336M/10.3G [00:12<02:39, 62.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  14% 377M/2.78G [00:12<00:48, 49.2MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  66% 325M/492M [00:12<00:03, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 346M/10.3G [00:12<02:29, 66.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 304M/335M [00:12<00:00, 55.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  97% 325M/335M [00:12<00:00, 54.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  68% 336M/492M [00:12<00:02, 61.0MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  14% 388M/2.78G [00:12<00:49, 48.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:12<00:00, 48.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:12<00:00, 27.0MB/s]\n",
            "\n",
            "\n",
            "model.safetensors:  70% 346M/492M [00:12<00:02, 53.4MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  14% 398M/2.78G [00:12<00:51, 46.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  97% 325M/335M [00:12<00:00, 56.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  72% 357M/492M [00:13<00:02, 55.4MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  15% 409M/2.78G [00:13<00:51, 45.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:12<00:00, 50.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 377M/10.3G [00:12<03:00, 54.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:12<00:00, 26.0MB/s]\n",
            "\n",
            "model.safetensors:  15% 419M/2.78G [00:13<00:48, 49.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 388M/10.3G [00:13<02:44, 60.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79% 388M/492M [00:13<00:01, 73.3MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  15% 430M/2.78G [00:13<00:40, 57.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 398M/10.3G [00:13<02:47, 59.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81% 398M/492M [00:13<00:01, 73.1MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  16% 440M/2.78G [00:13<00:40, 57.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 409M/10.3G [00:13<02:41, 61.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83% 409M/492M [00:13<00:01, 75.3MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  16% 451M/2.78G [00:13<00:36, 64.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 419M/10.3G [00:13<02:23, 68.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85% 419M/492M [00:13<00:00, 76.4MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  17% 461M/2.78G [00:13<00:35, 65.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 430M/10.3G [00:13<02:19, 70.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87% 430M/492M [00:13<00:00, 80.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89% 440M/492M [00:14<00:00, 82.6MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  17% 482M/2.78G [00:14<00:31, 72.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 451M/10.3G [00:13<02:12, 74.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92% 451M/492M [00:14<00:00, 78.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 461M/10.3G [00:14<02:11, 74.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94% 461M/492M [00:14<00:00, 78.8MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  18% 503M/2.78G [00:14<00:27, 81.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 472M/10.3G [00:14<02:03, 79.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96% 472M/492M [00:14<00:00, 77.2MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  18% 514M/2.78G [00:14<00:27, 81.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 482M/10.3G [00:14<01:58, 82.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  19% 524M/2.78G [00:14<00:26, 85.1MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  98% 482M/492M [00:14<00:00, 77.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 493M/10.3G [00:14<01:59, 81.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  19% 535M/2.78G [00:14<00:26, 86.1MB/s]\u001b[A\n",
            "\n",
            "model.safetensors: 100% 492M/492M [00:14<00:00, 33.4MB/s]\n",
            "Fetching 19 files:  21% 4/19 [00:14<01:05,  4.39s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching 19 files:  21% 4/19 [00:07<00:28,  1.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 503M/10.3G [00:14<02:00, 81.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  20% 545M/2.78G [00:14<00:26, 82.8MB/s]\u001b[A\n",
            "model.safetensors:  20% 566M/2.78G [00:15<00:22, 100MB/s] \u001b[A\n",
            "model.safetensors:  22% 598M/2.78G [00:15<00:15, 137MB/s]\u001b[A\n",
            "model.safetensors:  23% 629M/2.78G [00:15<00:12, 170MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 524M/10.3G [00:15<03:26, 47.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  23% 650M/2.78G [00:15<00:17, 123MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 535M/10.3G [00:15<03:12, 50.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  24% 671M/2.78G [00:15<00:19, 110MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 556M/10.3G [00:15<02:30, 64.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  25% 692M/2.78G [00:16<00:17, 116MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 577M/10.3G [00:15<02:05, 77.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  26% 713M/2.78G [00:16<00:17, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 598M/10.3G [00:15<01:45, 91.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  26% 734M/2.78G [00:16<00:16, 128MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 619M/10.3G [00:16<01:30, 106MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  27% 755M/2.78G [00:16<00:15, 132MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 640M/10.3G [00:16<01:23, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  28% 776M/2.78G [00:16<00:15, 131MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 661M/10.3G [00:16<01:23, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  29% 797M/2.78G [00:16<00:15, 129MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 682M/10.3G [00:16<01:20, 119MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  29% 818M/2.78G [00:16<00:15, 125MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 703M/10.3G [00:16<01:13, 131MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  30% 839M/2.78G [00:17<00:14, 132MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 724M/10.3G [00:16<01:12, 132MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  31% 860M/2.78G [00:17<00:15, 124MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 744M/10.3G [00:17<01:17, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 765M/10.3G [00:17<01:09, 136MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 786M/10.3G [00:17<01:06, 144MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  32% 881M/2.78G [00:17<00:18, 102MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 807M/10.3G [00:17<01:04, 147MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  32% 902M/2.78G [00:17<00:17, 107MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 828M/10.3G [00:17<01:08, 137MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  33% 923M/2.78G [00:17<00:16, 113MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 849M/10.3G [00:17<01:09, 135MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  34% 944M/2.78G [00:18<00:15, 115MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 870M/10.3G [00:17<01:10, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  35% 965M/2.78G [00:18<00:15, 114MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 891M/10.3G [00:18<01:10, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  35% 986M/2.78G [00:18<00:15, 115MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 912M/10.3G [00:18<01:15, 124MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  36% 1.01G/2.78G [00:18<00:15, 115MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 933M/10.3G [00:18<01:16, 122MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  37% 1.03G/2.78G [00:18<00:14, 118MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 954M/10.3G [00:18<01:19, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  38% 1.05G/2.78G [00:18<00:14, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 975M/10.3G [00:18<01:16, 122MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  38% 1.07G/2.78G [00:19<00:14, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 996M/10.3G [00:18<01:23, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  39% 1.09G/2.78G [00:19<00:14, 116MB/s]\u001b[A\n",
            "model.safetensors:  40% 1.11G/2.78G [00:19<00:14, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 1.02G/10.3G [00:19<01:28, 104MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 1.04G/10.3G [00:19<01:21, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  41% 1.13G/2.78G [00:19<00:14, 112MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 1.06G/10.3G [00:19<01:12, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  42% 1.15G/2.78G [00:19<00:13, 123MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 1.08G/10.3G [00:19<01:14, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  42% 1.17G/2.78G [00:20<00:14, 115MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 1.10G/10.3G [00:19<01:21, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  43% 1.20G/2.78G [00:20<00:14, 111MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 1.12G/10.3G [00:20<01:28, 104MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  44% 1.22G/2.78G [00:20<00:14, 107MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 1.14G/10.3G [00:20<01:22, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  45% 1.24G/2.78G [00:20<00:14, 108MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 1.16G/10.3G [00:20<01:20, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  45% 1.26G/2.78G [00:20<00:12, 118MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 1.18G/10.3G [00:20<01:24, 108MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  46% 1.28G/2.78G [00:21<00:13, 109MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 1.21G/10.3G [00:20<01:18, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  47% 1.30G/2.78G [00:21<00:12, 122MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 1.23G/10.3G [00:20<01:15, 119MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  48% 1.32G/2.78G [00:21<00:12, 119MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 1.25G/10.3G [00:21<01:12, 124MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 1.27G/10.3G [00:21<01:09, 129MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  48% 1.34G/2.78G [00:21<00:14, 99.6MB/s]\u001b[A\n",
            "model.safetensors:  49% 1.36G/2.78G [00:21<00:13, 109MB/s] \u001b[A\n",
            "model.safetensors:  50% 1.39G/2.78G [00:21<00:10, 138MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 1.29G/10.3G [00:21<01:33, 96.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  51% 1.42G/2.78G [00:22<00:09, 147MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 1.31G/10.3G [00:21<01:22, 108MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  52% 1.44G/2.78G [00:22<00:08, 159MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 1.33G/10.3G [00:21<01:12, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  52% 1.46G/2.78G [00:22<00:08, 164MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 1.35G/10.3G [00:22<01:08, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  53% 1.48G/2.78G [00:22<00:07, 167MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 1.37G/10.3G [00:22<01:02, 142MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  54% 1.50G/2.78G [00:22<00:08, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 1.39G/10.3G [00:22<00:57, 156MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  55% 1.52G/2.78G [00:22<00:07, 161MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 1.42G/10.3G [00:22<00:54, 164MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  56% 1.55G/2.78G [00:22<00:06, 181MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 1.44G/10.3G [00:22<00:58, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 1.46G/10.3G [00:22<01:07, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  57% 1.57G/2.78G [00:23<00:09, 127MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 1.48G/10.3G [00:22<01:06, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  57% 1.59G/2.78G [00:23<00:08, 135MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 1.50G/10.3G [00:23<01:03, 138MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  58% 1.61G/2.78G [00:23<00:08, 133MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 1.52G/10.3G [00:23<00:58, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  59% 1.64G/2.78G [00:23<00:08, 136MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 1.54G/10.3G [00:23<00:59, 148MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  60% 1.66G/2.78G [00:23<00:07, 148MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 1.56G/10.3G [00:23<01:02, 139MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  60% 1.68G/2.78G [00:23<00:07, 141MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 1.58G/10.3G [00:23<01:03, 137MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  61% 1.70G/2.78G [00:23<00:07, 137MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 1.60G/10.3G [00:23<01:04, 135MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  62% 1.72G/2.78G [00:24<00:08, 129MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 1.63G/10.3G [00:25<05:12, 27.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  63% 1.74G/2.78G [00:26<00:37, 28.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 1.66G/10.3G [00:26<03:22, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  64% 1.77G/2.78G [00:26<00:23, 42.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 1.68G/10.3G [00:26<02:41, 53.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  65% 1.79G/2.78G [00:26<00:18, 53.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 1.70G/10.3G [00:26<02:09, 66.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  65% 1.81G/2.78G [00:26<00:14, 66.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 1.72G/10.3G [00:26<01:47, 79.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  66% 1.84G/2.78G [00:26<00:11, 79.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 1.74G/10.3G [00:26<01:31, 93.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  67% 1.86G/2.78G [00:27<00:10, 86.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 1.76G/10.3G [00:26<01:20, 106MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  68% 1.88G/2.78G [00:27<00:09, 97.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 1.78G/10.3G [00:26<01:12, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  68% 1.90G/2.78G [00:27<00:07, 111MB/s] \u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 1.80G/10.3G [00:26<01:03, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 1.82G/10.3G [00:27<00:57, 147MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  69% 1.92G/2.78G [00:27<00:07, 121MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 1.85G/10.3G [00:27<00:53, 158MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  70% 1.94G/2.78G [00:27<00:07, 116MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 1.87G/10.3G [00:27<00:52, 160MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  71% 1.96G/2.78G [00:27<00:07, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 1.89G/10.3G [00:27<01:02, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  71% 1.98G/2.78G [00:27<00:06, 127MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 1.91G/10.3G [00:27<01:04, 129MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  72% 2.00G/2.78G [00:28<00:05, 138MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 1.93G/10.3G [00:27<01:00, 137MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  73% 2.02G/2.78G [00:28<00:05, 131MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 1.95G/10.3G [00:27<01:02, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  74% 2.04G/2.78G [00:28<00:05, 130MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 1.97G/10.3G [00:28<01:02, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  74% 2.07G/2.78G [00:28<00:05, 126MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 1.99G/10.3G [00:28<01:01, 135MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  75% 2.09G/2.78G [00:28<00:05, 123MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 2.01G/10.3G [00:28<01:02, 132MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  76% 2.11G/2.78G [00:28<00:05, 126MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 2.03G/10.3G [00:28<01:01, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  77% 2.13G/2.78G [00:29<00:06, 108MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 2.06G/10.3G [00:28<01:14, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  77% 2.15G/2.78G [00:29<00:05, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 2.08G/10.3G [00:29<01:07, 121MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  78% 2.17G/2.78G [00:29<00:04, 131MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 2.10G/10.3G [00:29<01:01, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  79% 2.19G/2.78G [00:29<00:05, 112MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 2.12G/10.3G [00:29<01:09, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  80% 2.21G/2.78G [00:29<00:04, 124MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 2.14G/10.3G [00:29<01:01, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 2.16G/10.3G [00:29<00:59, 136MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  80% 2.23G/2.78G [00:29<00:04, 125MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 2.18G/10.3G [00:29<01:03, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  81% 2.25G/2.78G [00:30<00:04, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 2.20G/10.3G [00:29<00:57, 140MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  82% 2.28G/2.78G [00:30<00:03, 128MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 2.22G/10.3G [00:30<01:02, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  83% 2.30G/2.78G [00:30<00:03, 128MB/s]\u001b[A\n",
            "model.safetensors:  83% 2.32G/2.78G [00:30<00:03, 134MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 2.24G/10.3G [00:30<01:04, 124MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  84% 2.34G/2.78G [00:30<00:03, 137MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 2.26G/10.3G [00:30<01:02, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  85% 2.36G/2.78G [00:30<00:03, 134MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 2.29G/10.3G [00:30<01:01, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  86% 2.38G/2.78G [00:31<00:03, 130MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 2.31G/10.3G [00:30<01:03, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  86% 2.40G/2.78G [00:31<00:02, 138MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 2.33G/10.3G [00:30<01:00, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 2.35G/10.3G [00:31<01:12, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  87% 2.42G/2.78G [00:32<00:05, 59.8MB/s]\u001b[A\n",
            "model.safetensors:  88% 2.44G/2.78G [00:32<00:04, 74.7MB/s]\u001b[A\n",
            "model.safetensors:  89% 2.46G/2.78G [00:32<00:03, 90.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 2.37G/10.3G [00:31<02:13, 59.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 2.39G/10.3G [00:32<01:54, 69.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  89% 2.49G/2.78G [00:32<00:03, 93.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 2.41G/10.3G [00:32<01:35, 82.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  90% 2.51G/2.78G [00:32<00:02, 102MB/s] \u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 2.43G/10.3G [00:32<01:26, 90.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  91% 2.53G/2.78G [00:32<00:02, 110MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 2.45G/10.3G [00:32<01:20, 96.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  92% 2.55G/2.78G [00:32<00:02, 113MB/s]\u001b[A\n",
            "model.safetensors:  92% 2.57G/2.78G [00:33<00:01, 118MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 2.47G/10.3G [00:32<01:15, 103MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 2.50G/10.3G [00:32<01:11, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  93% 2.59G/2.78G [00:33<00:01, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 2.52G/10.3G [00:33<01:05, 119MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 2.55G/10.3G [00:33<00:53, 144MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  94% 2.61G/2.78G [00:33<00:01, 91.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 2.57G/10.3G [00:33<00:51, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  95% 2.63G/2.78G [00:33<00:01, 100MB/s] \u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 2.59G/10.3G [00:33<00:53, 143MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  95% 2.65G/2.78G [00:33<00:01, 108MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 2.61G/10.3G [00:33<00:55, 139MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  96% 2.67G/2.78G [00:34<00:00, 113MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 2.63G/10.3G [00:33<00:56, 135MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  97% 2.69G/2.78G [00:34<00:00, 111MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 2.65G/10.3G [00:34<00:55, 137MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  98% 2.72G/2.78G [00:34<00:00, 119MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 2.67G/10.3G [00:34<00:58, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 2.69G/10.3G [00:34<00:57, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  98% 2.74G/2.78G [00:34<00:00, 114MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 2.72G/10.3G [00:34<00:52, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  27% 2.75G/10.3G [00:34<00:42, 177MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  27% 2.78G/10.3G [00:34<00:36, 208MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  99% 2.76G/2.78G [00:35<00:00, 77.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  27% 2.81G/10.3G [00:34<00:46, 162MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors: 100% 2.78G/2.78G [00:35<00:00, 78.5MB/s]\n",
            "Fetching 19 files:  32% 6/19 [00:35<01:32,  7.09s/it]\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  28% 2.83G/10.3G [00:35<00:48, 155MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching 19 files:  32% 6/19 [00:28<01:10,  5.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  28% 2.85G/10.3G [00:35<00:45, 162MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  28% 2.88G/10.3G [00:35<00:40, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  28% 2.92G/10.3G [00:35<00:35, 204MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  29% 2.95G/10.3G [00:35<00:33, 218MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  29% 2.98G/10.3G [00:35<00:35, 207MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  29% 3.01G/10.3G [00:35<00:36, 197MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 3.04G/10.3G [00:37<02:26, 49.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 3.06G/10.3G [00:40<05:21, 22.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 3.10G/10.3G [00:40<03:22, 35.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 3.14G/10.3G [00:40<02:30, 47.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 3.16G/10.3G [00:40<02:07, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 3.18G/10.3G [00:40<01:44, 67.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 3.21G/10.3G [00:40<01:18, 89.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 3.24G/10.3G [00:41<01:02, 113MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 3.27G/10.3G [00:41<00:50, 139MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 3.30G/10.3G [00:41<00:42, 163MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 3.33G/10.3G [00:41<00:37, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  33% 3.37G/10.3G [00:41<00:34, 201MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  33% 3.40G/10.3G [00:41<00:31, 216MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  33% 3.43G/10.3G [00:41<00:30, 227MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 3.46G/10.3G [00:41<00:29, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 3.49G/10.3G [00:42<00:29, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 3.52G/10.3G [00:42<00:28, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  35% 3.55G/10.3G [00:42<00:28, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  35% 3.59G/10.3G [00:42<00:27, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  35% 3.62G/10.3G [00:42<00:27, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  36% 3.65G/10.3G [00:42<00:27, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  36% 3.68G/10.3G [00:42<00:26, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  36% 3.71G/10.3G [00:42<00:26, 249MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  36% 3.74G/10.3G [00:43<00:26, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  37% 3.77G/10.3G [00:43<00:26, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  37% 3.81G/10.3G [00:43<00:25, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  37% 3.84G/10.3G [00:43<00:25, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 3.87G/10.3G [00:43<00:26, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 3.90G/10.3G [00:43<00:25, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 3.93G/10.3G [00:43<00:25, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  39% 3.96G/10.3G [00:43<00:25, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  39% 4.00G/10.3G [00:44<00:25, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  39% 4.03G/10.3G [00:44<00:25, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  40% 4.06G/10.3G [00:44<00:25, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  40% 4.09G/10.3G [00:44<00:24, 251MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  40% 4.12G/10.3G [00:44<00:24, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  40% 4.15G/10.3G [00:45<00:45, 135MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 4.18G/10.3G [00:45<00:38, 157MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 4.22G/10.3G [00:45<00:34, 176MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 4.25G/10.3G [00:45<00:32, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  42% 4.28G/10.3G [00:45<00:31, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  42% 4.31G/10.3G [00:45<00:27, 217MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  42% 4.34G/10.3G [00:45<00:25, 229MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  43% 4.37G/10.3G [00:45<00:25, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  43% 4.40G/10.3G [00:46<00:25, 227MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  43% 4.44G/10.3G [00:46<00:24, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  43% 4.47G/10.3G [00:46<00:23, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 4.50G/10.3G [00:46<00:23, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 4.53G/10.3G [00:46<00:24, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 4.56G/10.3G [00:46<00:24, 229MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  45% 4.59G/10.3G [00:46<00:23, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  45% 4.62G/10.3G [00:47<00:22, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  45% 4.66G/10.3G [00:47<00:23, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  46% 4.69G/10.3G [00:47<00:24, 232MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  46% 4.72G/10.3G [00:47<00:24, 231MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  46% 4.75G/10.3G [00:47<00:23, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 4.78G/10.3G [00:47<00:24, 228MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 4.81G/10.3G [00:47<00:24, 225MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 4.84G/10.3G [00:47<00:22, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 4.88G/10.3G [00:48<00:21, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  48% 4.91G/10.3G [00:48<00:25, 211MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  48% 4.94G/10.3G [00:48<00:27, 197MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  48% 4.96G/10.3G [00:48<00:27, 196MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  48% 4.98G/10.3G [00:49<01:06, 79.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  49% 5.00G/10.3G [00:49<00:56, 94.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  49% 5.02G/10.3G [00:49<00:48, 109MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  49% 5.04G/10.3G [00:49<00:41, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  49% 5.08G/10.3G [00:49<00:35, 146MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 5.10G/10.3G [00:49<00:33, 156MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 5.14G/10.3G [00:50<00:26, 196MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 5.17G/10.3G [00:50<00:24, 212MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  51% 5.20G/10.3G [00:50<00:23, 217MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  51% 5.23G/10.3G [00:50<00:22, 222MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  51% 5.26G/10.3G [00:50<00:22, 218MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  52% 5.30G/10.3G [00:50<00:23, 210MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  52% 5.33G/10.3G [00:50<00:24, 200MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  52% 5.35G/10.3G [00:53<02:13, 37.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  52% 5.37G/10.3G [00:54<03:06, 26.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 5.40G/10.3G [00:54<02:08, 38.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 5.43G/10.3G [00:54<01:31, 52.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 5.46G/10.3G [00:55<01:08, 69.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  54% 5.49G/10.3G [00:55<00:53, 88.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  54% 5.53G/10.3G [00:55<00:42, 113MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  54% 5.56G/10.3G [00:55<00:34, 135MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  54% 5.59G/10.3G [00:55<00:30, 151MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  55% 5.62G/10.3G [00:55<00:26, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  55% 5.65G/10.3G [00:55<00:23, 195MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  55% 5.68G/10.3G [00:55<00:22, 208MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 5.71G/10.3G [00:56<00:20, 219MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 5.75G/10.3G [00:56<00:20, 223MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 5.78G/10.3G [01:00<03:31, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  57% 5.81G/10.3G [01:00<02:31, 29.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  57% 5.84G/10.3G [01:01<01:50, 39.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  57% 5.87G/10.3G [01:01<01:23, 52.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  57% 5.90G/10.3G [01:01<01:03, 68.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  58% 5.93G/10.3G [01:01<00:48, 88.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  58% 5.97G/10.3G [01:01<00:38, 112MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  58% 6.00G/10.3G [01:01<00:32, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 6.03G/10.3G [01:02<00:42, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 6.05G/10.3G [01:04<02:11, 32.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 6.07G/10.3G [01:04<02:02, 34.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 6.11G/10.3G [01:04<01:17, 53.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 6.14G/10.3G [01:05<00:58, 70.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 6.17G/10.3G [01:05<00:50, 81.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 6.19G/10.3G [01:05<00:43, 94.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 6.21G/10.3G [01:05<00:37, 107MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  61% 6.24G/10.3G [01:05<00:29, 136MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  61% 6.27G/10.3G [01:05<00:23, 167MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  61% 6.30G/10.3G [01:05<00:20, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  62% 6.33G/10.3G [01:05<00:18, 210MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  62% 6.36G/10.3G [01:06<00:17, 220MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  62% 6.40G/10.3G [01:06<00:16, 229MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 6.43G/10.3G [01:06<00:16, 233MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 6.46G/10.3G [01:06<00:16, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 6.49G/10.3G [01:06<00:15, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 6.52G/10.3G [01:06<00:15, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 6.55G/10.3G [01:06<00:15, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 6.59G/10.3G [01:06<00:16, 227MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 6.62G/10.3G [01:07<00:15, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  65% 6.65G/10.3G [01:07<00:14, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  65% 6.68G/10.3G [01:07<00:14, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  65% 6.71G/10.3G [01:07<00:14, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  66% 6.74G/10.3G [01:07<00:14, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  66% 6.77G/10.3G [01:07<00:14, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  66% 6.81G/10.3G [01:07<00:13, 252MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  67% 6.84G/10.3G [01:07<00:13, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  67% 6.87G/10.3G [01:08<00:13, 249MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  67% 6.90G/10.3G [01:08<00:13, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  67% 6.93G/10.3G [01:08<00:13, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  68% 6.96G/10.3G [01:08<00:13, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  68% 6.99G/10.3G [01:08<00:13, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  68% 7.03G/10.3G [01:08<00:13, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 7.06G/10.3G [01:08<00:13, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 7.09G/10.3G [01:09<00:13, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 7.12G/10.3G [01:09<00:12, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  70% 7.15G/10.3G [01:09<00:12, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  70% 7.18G/10.3G [01:09<00:21, 144MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  70% 7.22G/10.3G [01:09<00:17, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  71% 7.26G/10.3G [01:09<00:15, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  71% 7.29G/10.3G [01:10<00:15, 196MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  71% 7.32G/10.3G [01:10<00:13, 216MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 7.35G/10.3G [01:10<00:12, 228MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 7.38G/10.3G [01:10<00:12, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 7.41G/10.3G [01:10<00:12, 227MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 7.44G/10.3G [01:10<00:11, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  73% 7.48G/10.3G [01:10<00:11, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  73% 7.51G/10.3G [01:10<00:11, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  73% 7.54G/10.3G [01:11<00:11, 235MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  74% 7.57G/10.3G [01:11<00:24, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  74% 7.59G/10.3G [01:12<00:45, 58.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  74% 7.61G/10.3G [01:15<01:48, 24.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  74% 7.64G/10.3G [01:15<01:13, 35.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 7.68G/10.3G [01:15<00:52, 49.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 7.70G/10.3G [01:15<00:43, 59.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 7.72G/10.3G [01:15<00:35, 71.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 7.74G/10.3G [01:15<00:30, 84.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  76% 7.77G/10.3G [01:15<00:23, 107MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  76% 7.80G/10.3G [01:16<00:18, 136MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  76% 7.83G/10.3G [01:16<00:14, 164MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 7.86G/10.3G [01:16<00:14, 171MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 7.90G/10.3G [01:16<00:14, 168MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 7.92G/10.3G [01:16<00:13, 170MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 7.94G/10.3G [01:16<00:20, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 7.96G/10.3G [01:21<02:21, 16.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  78% 7.99G/10.3G [01:21<01:32, 24.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  78% 8.01G/10.3G [01:21<01:15, 29.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  78% 8.04G/10.3G [01:21<00:50, 43.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  79% 8.06G/10.3G [01:21<00:40, 54.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  79% 8.10G/10.3G [01:22<00:29, 74.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  79% 8.14G/10.3G [01:22<00:20, 106MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  80% 8.17G/10.3G [01:22<00:16, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  80% 8.20G/10.3G [01:22<00:14, 147MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  80% 8.23G/10.3G [01:22<00:12, 168MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  80% 8.26G/10.3G [01:22<00:10, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 8.29G/10.3G [01:22<00:09, 200MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 8.33G/10.3G [01:23<00:09, 211MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 8.36G/10.3G [01:23<00:08, 219MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  82% 8.39G/10.3G [01:23<00:08, 228MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  82% 8.42G/10.3G [01:27<01:17, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  82% 8.46G/10.3G [01:27<00:50, 36.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  83% 8.49G/10.3G [01:27<00:37, 47.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  83% 8.52G/10.3G [01:27<00:28, 61.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  83% 8.56G/10.3G [01:27<00:22, 77.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  84% 8.59G/10.3G [01:28<00:17, 98.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  84% 8.62G/10.3G [01:28<00:13, 119MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  84% 8.65G/10.3G [01:28<00:12, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 8.68G/10.3G [01:28<00:10, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 8.71G/10.3G [01:28<00:09, 171MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 8.75G/10.3G [01:28<00:07, 191MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 8.78G/10.3G [01:28<00:07, 204MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  86% 8.81G/10.3G [01:28<00:07, 200MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  86% 8.84G/10.3G [01:29<00:09, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  86% 8.86G/10.3G [01:33<01:12, 19.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  87% 8.90G/10.3G [01:33<00:44, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  87% 8.93G/10.3G [01:33<00:32, 40.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  87% 8.97G/10.3G [01:34<00:24, 54.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 9.00G/10.3G [01:34<00:18, 68.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 9.03G/10.3G [01:34<00:14, 88.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 9.06G/10.3G [01:34<00:10, 112MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  89% 9.09G/10.3G [01:34<00:08, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  89% 9.12G/10.3G [01:34<00:07, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  89% 9.15G/10.3G [01:34<00:06, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  89% 9.19G/10.3G [01:35<00:05, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  90% 9.22G/10.3G [01:35<00:05, 203MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  90% 9.25G/10.3G [01:35<00:04, 214MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  90% 9.28G/10.3G [01:35<00:04, 222MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 9.31G/10.3G [01:35<00:04, 229MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 9.34G/10.3G [01:35<00:03, 233MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 9.37G/10.3G [01:35<00:03, 235MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  92% 9.41G/10.3G [01:35<00:03, 226MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  92% 9.44G/10.3G [01:36<00:03, 227MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  92% 9.47G/10.3G [01:36<00:03, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  93% 9.50G/10.3G [01:36<00:05, 147MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  93% 9.52G/10.3G [01:37<00:07, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  93% 9.55G/10.3G [01:37<00:05, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  93% 9.58G/10.3G [01:37<00:04, 149MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  94% 9.62G/10.3G [01:37<00:03, 168MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  94% 9.65G/10.3G [01:37<00:03, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  94% 9.68G/10.3G [01:37<00:02, 204MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 9.71G/10.3G [01:37<00:02, 196MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 9.74G/10.3G [01:37<00:02, 214MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 9.77G/10.3G [01:38<00:02, 229MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 9.80G/10.3G [01:38<00:02, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  96% 9.84G/10.3G [01:38<00:01, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  96% 9.87G/10.3G [01:38<00:01, 231MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  96% 9.90G/10.3G [01:38<00:01, 234MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  97% 9.93G/10.3G [01:38<00:01, 249MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  97% 9.96G/10.3G [01:38<00:01, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  97% 9.99G/10.3G [01:39<00:01, 204MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  98% 10.0G/10.3G [01:39<00:01, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  98% 10.1G/10.3G [01:39<00:01, 197MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  98% 10.1G/10.3G [01:39<00:00, 229MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  99% 10.1G/10.3G [01:39<00:00, 233MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  99% 10.2G/10.3G [01:39<00:00, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  99% 10.2G/10.3G [01:39<00:00, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 10.2G/10.3G [01:40<00:00, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 10.3G/10.3G [01:40<00:00, 102MB/s]\n",
            "Fetching 19 files: 100% 19/19 [01:40<00:00,  5.30s/it]\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching 19 files: 100% 19/19 [01:33<00:00,  4.92s/it]\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "Loading pipeline components...:  14% 1/7 [00:04<00:24,  4.01s/it]\n",
            "Loading pipeline components...:  86% 6/7 [00:07<00:00,  1.03it/s]\n",
            "Loading pipeline components...: 100% 7/7 [00:07<00:00,  1.05s/it]\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:07<00:02,  1.16s/it]\u001b[A\n",
            "Loading pipeline components...: 100% 7/7 [00:07<00:00,  1.11s/it]\n",
            "DARRE2.safetensors: 100% 28.9M/28.9M [00:00<00:00, 40.8MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.0.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.1.attentions.1.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.0.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for down_blocks.2.attentions.1.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.0.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.1.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.0.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for up_blocks.1.attentions.2.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.proj_in.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.proj_in.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.1.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.2.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.3.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.4.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.5.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.6.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.8.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_q.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_q.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_k.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_k.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_v.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_v.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.ff.net.2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.ff.net.2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.proj_out.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.proj_out.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.0.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.1.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.2.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.4.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.5.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.3.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.6.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.8.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.7.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.9.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.10.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.k_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.k_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.v_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.v_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.q_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.q_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.out_proj.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.self_attn.out_proj.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc1.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc1.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc2.lora_A.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for text_model.encoder.layers.11.mlp.fc2.lora_B.default_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "2024-11-09 16:51:38.521 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 137, in <module>\n",
            "    outfit_image = generate_outfit_image(outfit_suggestion)\n",
            "  File \"/content/app.py\", line 42, in generate_outfit_image\n",
            "    pipe.load_lora_weights(\"Pinguin/luisap-not-dalle2-27mb\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 567, in load_lora_weights\n",
            "    self.load_lora_into_text_encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 846, in load_lora_into_text_encoder\n",
            "    text_encoder.to(device=text_encoder.device, dtype=text_encoder.dtype)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2905, in to\n",
            "    return super().to(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 5 more times]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1333, in convert\n",
            "    raise NotImplementedError(\n",
            "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
            "2024-11-09 16:51:38.523 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 137, in <module>\n",
            "    outfit_image = generate_outfit_image(outfit_suggestion)\n",
            "  File \"/content/app.py\", line 42, in generate_outfit_image\n",
            "    pipe.load_lora_weights(\"Pinguin/luisap-not-dalle2-27mb\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 567, in load_lora_weights\n",
            "    self.load_lora_into_text_encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 846, in load_lora_into_text_encoder\n",
            "    text_encoder.to(device=text_encoder.device, dtype=text_encoder.dtype)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2905, in to\n",
            "    return super().to(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 5 more times]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1333, in convert\n",
            "    raise NotImplementedError(\n",
            "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
            "2024-11-09 16:51:39.693 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  3.60it/s]\n",
            "2024-11-09 16:51:54.579 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 137, in <module>\n",
            "    outfit_image = generate_outfit_image(outfit_suggestion)\n",
            "  File \"/content/app.py\", line 42, in generate_outfit_image\n",
            "    pipe.load_lora_weights(\"Pinguin/luisap-not-dalle2-27mb\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 567, in load_lora_weights\n",
            "    self.load_lora_into_text_encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 846, in load_lora_into_text_encoder\n",
            "    text_encoder.to(device=text_encoder.device, dtype=text_encoder.dtype)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2905, in to\n",
            "    return super().to(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 5 more times]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1333, in convert\n",
            "    raise NotImplementedError(\n",
            "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
            "2024-11-09 16:55:40.748 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  5.07it/s]\n",
            "2024-11-09 16:56:02.673 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 137, in <module>\n",
            "    st.markdown(f\"[Buy this outfit: {shopping_link}]({shopping_link})\", unsafe_allow_html=True)\n",
            "  File \"/content/app.py\", line 42, in generate_outfit_image\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 567, in load_lora_weights\n",
            "    self.load_lora_into_text_encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/loaders/lora_pipeline.py\", line 846, in load_lora_into_text_encoder\n",
            "    text_encoder.to(device=text_encoder.device, dtype=text_encoder.dtype)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2905, in to\n",
            "    return super().to(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 5 more times]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1333, in convert\n",
            "    raise NotImplementedError(\n",
            "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
            "2024-11-09 17:03:30.851 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2024-11-09 17:03:38.790 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import streamlit as st\n",
        "# import os\n",
        "# from groq import Groq\n",
        "\n",
        "# # Set the Groq API Key\n",
        "# GROQ_API_KEY = \"-----------------------\"  # Replace with your Groq API key\n",
        "\n",
        "# # Function to get weather data from OpenWeatherMap\n",
        "# def get_weather_data(city):\n",
        "#     api_key = \"--------------------\"  # Replace with your OpenWeatherMap API key\n",
        "#     url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
        "#     response = requests.get(url)\n",
        "#     return response.json()\n",
        "\n",
        "# # Function to parse weather data\n",
        "# def parse_weather_data(weather_data):\n",
        "#     temperature = weather_data[\"main\"][\"temp\"]\n",
        "#     weather_description = weather_data[\"weather\"][0][\"description\"]\n",
        "#     return temperature, weather_description\n",
        "\n",
        "# # Function to get outfit suggestion using Groq's LLaMA model\n",
        "# def get_outfit_suggestion(temperature, description):\n",
        "#     # Initialize Groq's API\n",
        "#     client = Groq(api_key=GROQ_API_KEY)  # Pass the API key\n",
        "\n",
        "#     prompt = f\"The current weather is {description} with a temperature of {temperature}°C. Suggest an outfit.\"\n",
        "\n",
        "#     # Use Groq's chat API for text generation\n",
        "#     chat_completion = client.chat.completions.create(\n",
        "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "#         model=\"llama3-8b-8192\",  # Replace with the correct model if needed\n",
        "#     )\n",
        "\n",
        "#     return chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "# # Streamlit UI for user input\n",
        "# st.title(\"Weather-Based Outfit Suggestion App\")\n",
        "\n",
        "# city = st.text_input(\"Enter your location:\")\n",
        "\n",
        "# if city:\n",
        "#     weather_data = get_weather_data(city)\n",
        "\n",
        "#     if weather_data[\"cod\"] == 200:\n",
        "#         temperature, description = parse_weather_data(weather_data)\n",
        "\n",
        "#         # Display current weather info\n",
        "#         st.write(f\"Current temperature in {city}: {temperature}°C\")\n",
        "#         st.write(f\"Weather: {description}\")\n",
        "\n",
        "#         # Get outfit suggestion from Groq's LLaMA model\n",
        "#         outfit_suggestion = get_outfit_suggestion(temperature, description)\n",
        "\n",
        "#         # Display outfit suggestion\n",
        "#         st.write(\"Outfit Suggestion:\")\n",
        "#         st.write(outfit_suggestion)\n",
        "\n",
        "#         # Display weather icon\n",
        "#         icon_code = weather_data[\"weather\"][0][\"icon\"]\n",
        "#         icon_url = f\"http://openweathermap.org/img/wn/{icon_code}.png\"\n",
        "#         st.image(icon_url)\n",
        "#     else:\n",
        "#         st.write(\"Could not retrieve weather data. Please check the location.\")\n",
        "\n",
        "# # Optional: Add CSS for styling\n",
        "# st.markdown(\n",
        "#     \"\"\"\n",
        "#     <style>\n",
        "#     .reportview-container {\n",
        "#         background: #f5f5f5;\n",
        "#     }\n",
        "#     .stButton>button {\n",
        "#         background-color: #ff5733;\n",
        "#         color: white;\n",
        "#     }\n",
        "#     </style>\n",
        "#     \"\"\",\n",
        "#     unsafe_allow_html=True\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "import streamlit as st\n",
        "import groq\n",
        "\n",
        "# Function to get weather data from OpenWeatherMap\n",
        "def get_weather_data(city):\n",
        "    api_key = \"---------------------------\"  # Replace with your OpenWeatherMap API key\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "# Function to parse weather data\n",
        "def parse_weather_data(weather_data):\n",
        "    temperature = weather_data[\"main\"][\"temp\"]\n",
        "    weather_description = weather_data[\"weather\"][0][\"description\"]\n",
        "    return temperature, weather_description\n",
        "\n",
        "# Function to get outfit suggestion using Groq's LLaMA model\n",
        "def get_outfit_suggestion(temperature, description, style, fabric):\n",
        "    # Initialize Groq's API\n",
        "    client = groq.Groq(api_key=\"------------------------------------------\")  # Replace with your Groq API key\n",
        "\n",
        "    prompt = f\"The current weather is {description} with a temperature of {temperature}°C. Suggest an outfit. The user prefers a {style} style and {fabric} fabric.\"\n",
        "\n",
        "    # Use Groq's chat completion to get the text response\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",  # You can change to a different model if necessary\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Streamlit UI for user input\n",
        "st.title(\"Weather-Based Outfit Suggestion App\")\n",
        "\n",
        "city = st.text_input(\"Enter your location:\")\n",
        "\n",
        "# Add style and fabric input options\n",
        "style = st.selectbox(\"Select your preferred style\", [\"Casual\", \"Formal\", \"Sporty\", \"Business\", \"Chic\"])\n",
        "fabric = st.selectbox(\"Select your preferred fabric\", [\"Cotton\", \"Linen\", \"Wool\", \"Polyester\", \"Silk\", \"Leather\"])\n",
        "\n",
        "if city:\n",
        "    weather_data = get_weather_data(city)\n",
        "\n",
        "    if weather_data[\"cod\"] == 200:\n",
        "        temperature, description = parse_weather_data(weather_data)\n",
        "\n",
        "        # Display current weather info\n",
        "        st.write(f\"Current temperature in {city}: {temperature}°C\")\n",
        "        st.write(f\"Weather: {description}\")\n",
        "\n",
        "        # Get outfit suggestion based on user preferences\n",
        "        outfit_suggestion = get_outfit_suggestion(temperature, description, style, fabric)\n",
        "\n",
        "        # Display outfit suggestion\n",
        "        st.write(\"Outfit Suggestion:\")\n",
        "        st.write(outfit_suggestion)\n",
        "\n",
        "        # Display weather icon\n",
        "        icon_code = weather_data[\"weather\"][0][\"icon\"]\n",
        "        icon_url = f\"http://openweathermap.org/img/wn/{icon_code}.png\"\n",
        "        st.image(icon_url)\n",
        "    else:\n",
        "        st.write(\"Could not retrieve weather data. Please check the location.\")\n",
        "\n",
        "# Optional: Add CSS for styling\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .reportview-container {\n",
        "        background: #f5f5f5;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        background-color: #ff5733;\n",
        "        color: white;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "vvro7d8_YWnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ul0pxOimwjEr",
        "outputId": "961f6e50-8a81-41b2-f7aa-ddda4af8649d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.30.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.26.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (10.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D-3Bhy9xWFY",
        "outputId": "c9f16e51-f387-4969-a3bf-60b2a89832a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained Stable Diffusion model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "\n",
        "# Check if GPU is available and use it, otherwise fall back to CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "pipe.to(device)\n",
        "\n",
        "# Example of generating an image\n",
        "prompt = \"A beautiful sunset over the mountains\"\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "# Show the image (optional in a Jupyter notebook)\n",
        "image.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "71c9267c227048c0a127332c934c596f",
            "b0b4c4c005124d63b84c9a77730a00ef",
            "c4160269801b4c21a3df194c3860592d",
            "342f9da828454a008c56d822263a279b",
            "1d0e163975d34d7bb8b3826af2153584",
            "192209c101cb407da5a6116d08551a35",
            "9ea8559f8d3a4d0fa05ccbba602d23c1",
            "c4425695aff847fa8258a328d5b4aaa4",
            "03e7a17e9ecd4981a15c00e081ca8efb",
            "4bc13f08a0944eb9aeb130d9df62f647",
            "612f262802b9475daf53dc63dc419b6a",
            "c19cde8d90f34f159bf36a2ce7339cb5",
            "7b6f070ceffa48a782bf36dd485eff79",
            "77612db84eab4edbb835ae0cf313d992",
            "2eee9d3cbf2143aeb8215e03a6416ca7",
            "e44721a5fed04e689f73292b286e5dad",
            "264274334b6e48e4b7e510b924a80b3b",
            "ba13ff27f8e342c486a58ab92f11c123",
            "fafb80046e704d65b49846d7303d10ce",
            "c6251ea311094901b5a55aa0b2abea7d",
            "dcdd2d34c1f24bf4901a1267fcf6e671",
            "a17517ed26654e038d9e807fa37d1cff"
          ]
        },
        "id": "MKZJM4_TwoeZ",
        "outputId": "811dd100-9d4d-4835-dae5-19808bc1d0a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71c9267c227048c0a127332c934c596f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c19cde8d90f34f159bf36a2ce7339cb5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFGA5ylyz-4_",
        "outputId": "556c9e83-571d-4a53-97c3-9cc568378b11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import streamlit as st\n",
        "# from diffusers import DiffusionPipeline\n",
        "# import torch\n",
        "# import groq\n",
        "\n",
        "# # Function to get weather data from OpenWeatherMap\n",
        "# def get_weather_data(city):\n",
        "#     api_key = \"-------------------------------------\"  # Replace with your OpenWeatherMap API key\n",
        "#     url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
        "#     response = requests.get(url)\n",
        "#     return response.json()\n",
        "\n",
        "# # Function to parse weather data\n",
        "# def parse_weather_data(weather_data):\n",
        "#     temperature = weather_data[\"main\"][\"temp\"]\n",
        "#     weather_description = weather_data[\"weather\"][0][\"description\"]\n",
        "#     return temperature, weather_description\n",
        "\n",
        "# # Function to get outfit suggestion using Groq's LLaMA model\n",
        "# def get_outfit_suggestion(temperature, description, style, fabric, gender):\n",
        "#     # Initialize Groq's API\n",
        "#     client = groq.Groq(api_key=\"-------------------------------------------\")  # Replace with your Groq API key\n",
        "\n",
        "#     prompt = f\"The current weather is {description} with a temperature of {temperature}°C. Suggest an outfit for a {gender} with {style} style and {fabric} fabric.\"\n",
        "\n",
        "#     # Use Groq's chat completion to get the text response\n",
        "#     response = client.chat.completions.create(\n",
        "#         messages=[{\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": prompt,\n",
        "#         }],\n",
        "#         model=\"llama3-8b-8192\",  # You can change to a different model if necessary\n",
        "#     )\n",
        "\n",
        "#     return response.choices[0].message.content.strip()\n",
        "\n",
        "# # Function to generate an outfit image based on the description\n",
        "# def generate_outfit_image(outfit_description):\n",
        "#     # Load the pre-trained model from Hugging Face\n",
        "#     pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "#     pipe.load_lora_weights(\"Pinguin/luisap-not-dalle2-27mb\")\n",
        "#     pipe.to(\"cuda\")  # Ensure the model runs on GPU, if available\n",
        "\n",
        "#     # Generate the image from the description\n",
        "#     image = pipe(outfit_description).images[0]\n",
        "\n",
        "#     return image\n",
        "\n",
        "# def get_shopping_links(outfit_suggestion):\n",
        "#     # Extract key items from the outfit suggestion\n",
        "#     keywords = []\n",
        "#     if \"cotton kameez\" in outfit_suggestion.lower():\n",
        "#         keywords.append(\"cotton kameez\")\n",
        "#     if \"pajama pants\" in outfit_suggestion.lower():\n",
        "#         keywords.append(\"pajama pants\")\n",
        "#     if \"shawl\" in outfit_suggestion.lower():\n",
        "#         keywords.append(\"cotton shawl\")\n",
        "#     if \"loafers\" in outfit_suggestion.lower():\n",
        "#         keywords.append(\"loafers\")\n",
        "#     if \"ballet flats\" in outfit_suggestion.lower():\n",
        "#         keywords.append(\"ballet flats\")\n",
        "#     if \"sandals\" in outfit_suggestion.lower():\n",
        "#         keywords.append(\"sandals\")\n",
        "\n",
        "#     # Generate search query based on extracted keywords\n",
        "#     search_query = '+'.join(keywords)\n",
        "#     shopping_url = f\"https://www.amazon.com/s?k={search_query.replace(' ', '+')}\"\n",
        "\n",
        "#     return shopping_url\n",
        "\n",
        "\n",
        "# # Streamlit UI for user input\n",
        "# st.title(\"Weather-Based Outfit Suggestion App\")\n",
        "\n",
        "# # Multilingual Support: Translate interface (Example: English & Spanish)\n",
        "# language = st.selectbox(\"Select Language\", [\"English\", \"Spanish\"])\n",
        "\n",
        "# if language == \"Spanish\":\n",
        "#     st.header(\"Sugerencia de Atuendo Basada en el Clima\")\n",
        "#     city_label = \"Ingrese su ubicación:\"\n",
        "#     style_label = \"Seleccione su estilo preferido\"\n",
        "#     fabric_label = \"Seleccione su tela preferida\"\n",
        "#     gender_label = \"Seleccione su género\"\n",
        "#     shopping_label = \"Encuentra atuendos aquí\"\n",
        "# else:\n",
        "#     st.header(\"Weather-Based Outfit Suggestion\")\n",
        "#     city_label = \"Enter your location:\"\n",
        "#     style_label = \"Select your preferred style\"\n",
        "#     fabric_label = \"Select your preferred fabric\"\n",
        "#     gender_label = \"Select your gender\"\n",
        "#     shopping_label = \"Find outfits here\"\n",
        "\n",
        "# # Sidebar: Left panel for customer selection\n",
        "# with st.sidebar:\n",
        "#     st.subheader(\"Customer Selection\")\n",
        "\n",
        "#     city = st.text_input(city_label, placeholder=\"Type your city...\")\n",
        "\n",
        "#     # Gender selection dropdown\n",
        "#     gender_options = [\"Male\", \"Female\", \"Other\"]\n",
        "#     gender = st.selectbox(gender_label, gender_options)\n",
        "\n",
        "#     # Style and fabric options\n",
        "#     style_options = [\"Casual\", \"Formal\", \"Sporty\", \"Business\", \"Chic\", \"Trendy\", \"Streetwear\", \"Bohemian\"]\n",
        "#     fabric_options = [\"Cotton\", \"Linen\", \"Wool\", \"Polyester\", \"Silk\", \"Leather\", \"Denim\", \"Rayon\"]\n",
        "\n",
        "#     style = st.selectbox(style_label, style_options)\n",
        "#     fabric = st.selectbox(fabric_label, fabric_options)\n",
        "\n",
        "#     # Custom style and fabric input\n",
        "#     custom_style = st.text_input(\"Or enter your own style (optional):\", placeholder=\"e.g. Vintage\")\n",
        "#     custom_fabric = st.text_input(\"Or enter your own fabric (optional):\", placeholder=\"e.g. Cashmere\")\n",
        "\n",
        "#     if custom_style:\n",
        "#         style = custom_style\n",
        "#     if custom_fabric:\n",
        "#         fabric = custom_fabric\n",
        "\n",
        "# # Main content area for displaying weather and outfit suggestions\n",
        "# if city:\n",
        "#     weather_data = get_weather_data(city)\n",
        "\n",
        "#     if weather_data[\"cod\"] == 200:\n",
        "#         temperature, description = parse_weather_data(weather_data)\n",
        "\n",
        "#         # Display current weather info with chatbot-like bubbles\n",
        "#         st.markdown(f\"<div class='chat-bubble'>Current temperature in {city}: {temperature}°C</div>\", unsafe_allow_html=True)\n",
        "#         st.markdown(f\"<div class='chat-bubble'>{description.capitalize()}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "#         # Get outfit suggestion based on user preferences, including gender\n",
        "#         outfit_suggestion = get_outfit_suggestion(temperature, description, style, fabric, gender)\n",
        "\n",
        "#         # Display outfit suggestion in a chat bubble\n",
        "#         st.markdown(f\"<div class='chat-bubble'><strong>Suggested Outfit:</strong> {outfit_suggestion}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "#         # Generate and display the outfit image\n",
        "#         outfit_image = generate_outfit_image(outfit_suggestion)\n",
        "#         st.image(outfit_image, caption=\"Generated Outfit Image\", use_column_width=True)\n",
        "\n",
        "#         # Get shopping link for the suggested outfit\n",
        "#         shopping_link = get_shopping_links(outfit_suggestion)\n",
        "#         st.markdown(f\"[Buy this outfit: {shopping_link}]({shopping_link})\", unsafe_allow_html=True)\n",
        "\n",
        "#         # Display weather icon\n",
        "#         icon_code = weather_data[\"weather\"][0][\"icon\"]\n",
        "#         icon_url = f\"http://openweathermap.org/img/wn/{icon_code}.png\"\n",
        "#         st.image(icon_url, width=50)\n",
        "\n",
        "#         # Weather alerts\n",
        "#         if temperature > 30:\n",
        "#             st.warning(f\"Warning: It's quite hot in {city}, stay hydrated!\")\n",
        "#         elif temperature < 10:\n",
        "#             st.warning(f\"Warning: It's cold in {city}, dress warmly!\")\n",
        "#     else:\n",
        "#         st.write(\"Could not retrieve weather data. Please check the location.\")\n",
        "\n",
        "# # Optional: Add CSS for styling\n",
        "# st.markdown(\n",
        "#     \"\"\"\n",
        "#     <style>\n",
        "#     .reportview-container {\n",
        "#         background: #e0f7fa;\n",
        "#         display: flex;\n",
        "#         flex-direction: column;\n",
        "#         justify-content: center;\n",
        "#         align-items: center;\n",
        "#     }\n",
        "\n",
        "#     .stButton>button {\n",
        "#         background-color: #ff5733;\n",
        "#         color: white;\n",
        "#         border-radius: 20px;\n",
        "#         padding: 10px 20px;\n",
        "#         font-size: 16px;\n",
        "#         transition: background-color 0.3s ease;\n",
        "#     }\n",
        "\n",
        "#     .stButton>button:hover {\n",
        "#         background-color: #e34b1f;\n",
        "#     }\n",
        "\n",
        "#     .stSelectbox {\n",
        "#         background-color: #f1f1f1;\n",
        "#         border-radius: 5px;\n",
        "#         width: 250px;\n",
        "#         font-size: 14px;\n",
        "#     }\n",
        "\n",
        "#     .chat-bubble {\n",
        "#         background-color: #ffffff;\n",
        "#         padding: 10px;\n",
        "#         border-radius: 10px;\n",
        "#         max-width: 70%;\n",
        "#         margin: 5px 0;\n",
        "#         box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\n",
        "#         font-size: 16px;\n",
        "#         font-family: 'Roboto', sans-serif;\n",
        "#     }\n",
        "\n",
        "#     h1, h2 {\n",
        "#         font-family: 'Roboto', sans-serif;\n",
        "#         color: #333333;\n",
        "#     }\n",
        "\n",
        "#     .stTextInput input {\n",
        "#         font-size: 14px;\n",
        "#     }\n",
        "#     </style>\n",
        "#     \"\"\", unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-h41jPL7vxwN",
        "outputId": "bc2ae9ab-63bc-4c57-e9d8-ebf53278010f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-09 16:26:12.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.145 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.148 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.165 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.166 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.167 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.182 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.183 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.190 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.191 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.194 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.200 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.202 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-09 16:26:12.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}